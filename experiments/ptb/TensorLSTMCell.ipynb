{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "import tensornet\n",
    "\n",
    "\n",
    "class TensorLSTMCell(RNNCell):\n",
    "    \"\"\"Basic LSTM recurrent network cell.\n",
    "\n",
    "    The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
    "\n",
    "    We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
    "    reduce the scale of forgetting in the beginning of the training.\n",
    "\n",
    "    It does not allow cell clipping, a projection layer, and does not\n",
    "    use peep-hole connections: it is the basic baseline.\n",
    "\n",
    "    For advanced models, please use the full LSTMCell that follows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, input_size=None,\n",
    "                 state_is_tuple=True, activation=tanh):\n",
    "        \"\"\"Initialize the basic LSTM cell.\n",
    "\n",
    "        Args:\n",
    "          num_units: int, The number of units in the LSTM cell.\n",
    "          forget_bias: float, The bias added to forget gates (see above).\n",
    "          input_size: Deprecated and unused.\n",
    "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "            the `c_state` and `m_state`.  If False, they are concatenated\n",
    "            along the column axis.  The latter behavior will soon be deprecated.\n",
    "          activation: Activation function of the inner states.\n",
    "        \"\"\"\n",
    "        if not state_is_tuple:\n",
    "            logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                         \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (LSTMStateTuple(self._num_units, self._num_units)\n",
    "                if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with vs.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "            # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = array_ops.split(1, 2, state)\n",
    "            \"\"\"Change to tensor model\"\"\"\n",
    "            tensornet.layers.tt(inputs, opts['inp_modes_1'],opts['out_modes_1'],new_opts['ranks_1'],=3.0, #0.1\n",
    "                                     'tt_' + str(len(layers)),use_biases=False)\n",
    "            #concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = array_ops.split(1, 4, concat)\n",
    "\n",
    "            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n",
    "                     self._activation(j))\n",
    "            new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = array_ops.concat(1, [new_c, new_h])\n",
    "            return new_h, new_state\n",
    "        \n",
    "        \n",
    "def _linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "    Args:\n",
    "      args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "      output_size: int, second dimension of W[i].\n",
    "      bias: boolean, whether to add a bias term or not.\n",
    "      bias_start: starting value to initialize the bias; 0 by default.\n",
    "      scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "\n",
    "    Returns:\n",
    "      A 2D Tensor with shape [batch x output_size] equal to\n",
    "      sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape().as_list() for a in args]\n",
    "    for shape in shapes:\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "        if not shape[1]:\n",
    "            raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "        else:\n",
    "            total_arg_size += shape[1]\n",
    "\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "\n",
    "    # Now the computation.\n",
    "    with vs.variable_scope(scope or \"Linear\"):\n",
    "        matrix = vs.get_variable(\n",
    "            \"Matrix\", [total_arg_size, output_size], dtype=dtype)\n",
    "        if len(args) == 1:\n",
    "            res = math_ops.matmul(args[0], matrix)\n",
    "        else:\n",
    "            res = math_ops.matmul(array_ops.concat(1, args), matrix)\n",
    "        if not bias:\n",
    "            return res\n",
    "        bias_term = vs.get_variable(\n",
    "            \"Bias\", [output_size],\n",
    "            dtype=dtype,\n",
    "            initializer=init_ops.constant_initializer(\n",
    "                bias_start, dtype=dtype))\n",
    "    return res + bias_term\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
