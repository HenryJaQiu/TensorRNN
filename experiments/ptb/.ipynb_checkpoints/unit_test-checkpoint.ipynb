{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cell \n",
    "$$ i, j, f, o = Wx + Uh + b \\\\\n",
    "   [i,j,f,o] = [W,U,1][x,h,b]^\\top$$\n",
    "  i = input_gate, j = new_input, f = forget_gate, o = output_gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<__main__.TensorBasicLSTMCell object at 0x7f4e300bf5d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs, should be 2 things one of length 4 and other of 6\n",
      "[[-0.20981316 -0.22256051 -0.14782372  0.10505638 -0.12265255 -0.00734006\n",
      "   0.21877538  0.19167691 -0.10904954 -0.09144102 -0.26857913  0.03122864]\n",
      " [-0.08267025 -0.05746111 -0.05679823  0.03828813 -0.0468681  -0.00637866\n",
      "   0.05214245  0.07274023 -0.03033189 -0.03075946 -0.10336353  0.01122909]]\n",
      "[[-0.2576791  -0.144777   -0.19867563  0.11424696 -0.19731012 -0.00094482\n",
      "   0.16024098  0.18690936 -0.11126959 -0.09403428 -0.20033431  0.03761324]\n",
      " [-0.20865364 -0.13724515 -0.14155808  0.08827887 -0.11730816 -0.00715122\n",
      "   0.13958806  0.18109582 -0.07854775 -0.0721107  -0.21904892  0.0268208 ]]\n",
      "[[-0.35850731 -0.24320629 -0.26680189  0.12561947 -0.22849809  0.00612856\n",
      "   0.29701939  0.33061045 -0.16770957 -0.15560433 -0.35192254  0.03802662]\n",
      " [-0.33554414 -0.24518134 -0.23367719  0.11091269 -0.19572376 -0.00677644\n",
      "   0.30411309  0.33196145 -0.15747315 -0.15440747 -0.38279125  0.03631699]]\n",
      "[[-0.37553197 -0.27953842 -0.31563342  0.08094294 -0.26975957  0.00134494\n",
      "   0.36483568  0.41008845 -0.19752441 -0.21540673 -0.48176003  0.02886987]\n",
      " [-0.35695443 -0.23842104 -0.29059163  0.08429497 -0.25936583 -0.01026437\n",
      "   0.30012485  0.35952592 -0.16275041 -0.1853409  -0.42933804  0.03376428]]\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.40006399 -0.19194576 -0.29611817  0.10878023 -0.29162458 -0.00270048\n",
      "   0.22623935  0.33442113 -0.14289591 -0.18236895 -0.32911903  0.04969701]]\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.40977004 -0.26562402 -0.32509732  0.08966671 -0.2889697  -0.00075986\n",
      "   0.34327638  0.43415838 -0.18198128 -0.23008674 -0.46275449  0.03824127]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "states, 2 things total both of size 2, which is the size of the hidden state\n",
      "[[ -1.20576525e+00  -6.70537472e-01  -6.65640593e-01   4.33468819e-01\n",
      "   -6.44789338e-01   2.79197050e-03   5.44194460e-01   8.35109234e-01\n",
      "   -3.59876931e-01  -3.84552419e-01  -8.07962060e-01   1.87043488e-01\n",
      "   -3.75531971e-01  -2.79538423e-01  -3.15633416e-01   8.09429437e-02\n",
      "   -2.69759566e-01   1.34494272e-03   3.64835680e-01   4.10088450e-01\n",
      "   -1.97524413e-01  -2.15406731e-01  -4.81760025e-01   2.88698711e-02]\n",
      " [ -1.34821773e+00  -7.20263004e-01  -6.96846664e-01   4.12599683e-01\n",
      "   -6.35175109e-01  -1.56959868e-03   5.54956615e-01   9.49391663e-01\n",
      "   -3.25275958e-01  -3.99110436e-01  -8.42942119e-01   2.05081344e-01\n",
      "   -4.09770042e-01  -2.65624017e-01  -3.25097322e-01   8.96667093e-02\n",
      "   -2.88969696e-01  -7.59861723e-04   3.43276381e-01   4.34158385e-01\n",
      "   -1.81981280e-01  -2.30086744e-01  -4.62754488e-01   3.82412709e-02]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "# %load unit_test.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "import tensornet\n",
    "from tensorflow.python.ops.rnn_cell import *\n",
    "\n",
    "\n",
    "class TensorBasicLSTMCell(LSTMCell):\n",
    "    \"\"\"Tensor Factorized Long short-term memory unit (LSTM) recurrent network cell.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_units, **kwargs):\n",
    "        super(TensorBasicLSTMCell, self).__init__(num_units)\n",
    "        self._inp_modes = kwargs['inp_modes']\n",
    "        self._out_modes = kwargs['out_modes']\n",
    "        self._mat_ranks = kwargs['mat_ranks']\n",
    "            \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with vs.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "            # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = array_ops.split(1, 2, state)     \n",
    "        \n",
    "            i = linear_tt([inputs, h], self._num_units, self._inp_modes, self._out_modes, self._mat_ranks, bias =True, scope = \"i\")  \n",
    "            j = linear_tt([inputs, h], self._num_units, self._inp_modes, self._out_modes, self._mat_ranks, bias =True, scope = \"j\")   \n",
    "            f = linear_tt([inputs, h], self._num_units, self._inp_modes, self._out_modes, self._mat_ranks, bias =True, scope = \"f\")   \n",
    "            o = linear_tt([inputs, h], self._num_units, self._inp_modes, self._out_modes, self._mat_ranks, bias =True, scope = \"o\")   \n",
    "        \n",
    "#             concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "#             # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "#             i , j, f, o = array_ops.split(1, 4, concat)\n",
    "\n",
    "            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n",
    "                     self._activation(j))\n",
    "            new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = array_ops.concat(1, [new_c, new_h])\n",
    "            return new_h, new_state\n",
    "\n",
    "def linear_tt(args, output_size, inp_modes, out_modes, mat_ranks, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"wrapper for factorization layer\"\"\"\n",
    "    # args = [x, h] solve y = Wx + Uh + b\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape().as_list() for a in args]\n",
    "    for shape in shapes:\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "        if not shape[1]:\n",
    "            raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "        else:\n",
    "            total_arg_size += shape[1]\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "\n",
    "    #sin = array_ops.concat(1, args)  batch_size* (x_dim + h_dim)\n",
    "    with vs.variable_scope(scope or \"Linear\"):\n",
    "#         matrix_x = vs.get_variable(\"Matrix_x\", [args[0].get_shape().as_list()[1], output_size])\n",
    "#         matrix_h = vs.get_variable(\"Matrix_h\", [args[1].get_shape().as_list()[1], output_size])\n",
    "#         res_x = math_ops.matmul(args[0], matrix_x) #tensornet.layers.tt(args[0], inp_modes['x'], out_modes['x'], mat_ranks['x'])\n",
    "#         res_h = math_ops.matmul(args[1], matrix_h)#tensornet.layers.tt(args[1], inp_modes['h'], out_modes['h'], mat_ranks['h'])\n",
    "#         res = res_x +  res_h #batch_size*out_size\n",
    "        res_x = tensornet.layers.mf_rnn(args[0],  inp_modes['x'], out_modes['x'], mat_ranks['x'], scope =\"x\")\n",
    "        res_h = tensornet.layers.mf_rnn(args[1],  inp_modes['h'], out_modes['h'], mat_ranks['h'], scope =\"h\")\n",
    "        res = res_x +  res_h\n",
    "        if not bias:\n",
    "            return res\n",
    "        bias_term = vs.get_variable(\"Bias\", [output_size],dtype=dtype,initializer=init_ops.constant_initializer(\n",
    "                bias_start, dtype=dtype))\n",
    "      \n",
    "    return res + bias_term\n",
    "\n",
    "\n",
    "    \n",
    "def _linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "    Args:\n",
    "      args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "      output_size: int, second dimension of W[i].\n",
    "      bias: boolean, whether to add a bias term or not.\n",
    "      bias_start: starting value to initialize the bias; 0 by default.\n",
    "      scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "\n",
    "    Returns:\n",
    "      A 2D Tensor with shape [batch x output_size] equal to\n",
    "      sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape().as_list() for a in args]\n",
    "    for shape in shapes:\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "        if not shape[1]:\n",
    "            raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "        else:\n",
    "            total_arg_size += shape[1]\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "\n",
    "    # Now the computation.\n",
    "    with vs.variable_scope(scope or \"Linear\"):\n",
    "        matrix = vs.get_variable(\n",
    "            \"Matrix\", [total_arg_size, output_size], dtype=dtype)\n",
    "        if len(args) == 1:\n",
    "            res = math_ops.matmul(args[0], matrix)\n",
    "        else:\n",
    "            res = math_ops.matmul(array_ops.concat(1, args), matrix)\n",
    "        if not bias:\n",
    "            return res\n",
    "        bias_term = vs.get_variable(\n",
    "            \"Bias\", [output_size],\n",
    "            dtype=dtype,\n",
    "            initializer=init_ops.constant_initializer(\n",
    "                bias_start, dtype=dtype))\n",
    "    return res + bias_term\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)\n",
    "    # the size of the hidden state for the lstm (notice the lstm uses 2x of this amount so actually lstm will have state of size 2)\n",
    "    size = 12\n",
    "    # 2 different sequences total\n",
    "    batch_size= 2\n",
    "    # the maximum steps for both sequences is 10\n",
    "    n_steps = 10\n",
    "    # each element of the sequence has dimension of 5\n",
    "    seq_width = 2\n",
    "\n",
    "    # the first input is to be stopped at 4 steps, the second at 6 steps\n",
    "    e_stop = np.array([4,6])\n",
    "    \n",
    "    # factorize the inp\n",
    "    inp_modes = {}\n",
    "    out_modes = {}\n",
    "    mat_ranks = {}\n",
    "    # input weights\n",
    "    inp_modes['x'] = np.array([1, 2, 1, 1], dtype='int32') # product as seq_width\n",
    "    out_modes['x'] = np.array([1, 4, 3, 1], dtype='int32') # product as num_units (size)\n",
    "    mat_ranks['x'] = 2\n",
    "    #mat_ranks['x'] = np.array([1, 2, 2, 2, 1], dtype='int32')\n",
    "    # hidden state weights\n",
    "    inp_modes['h'] = np.array([1, 4, 3, 1], dtype='int32') # seq_width\n",
    "    out_modes['h'] = np.array([1, 4, 3, 1], dtype='int32') # 4 * num_units\n",
    "    mat_ranks['h'] = 2\n",
    "\n",
    "    #mat_ranks['h'] = np.array([1, 2, 2, 2, 1], dtype='int32') \n",
    "    \n",
    "    \n",
    "    initializer = tf.random_uniform_initializer(-1,1)\n",
    "\n",
    "    # the sequences, has n steps of maximum size\n",
    "    seq_input = tf.placeholder(tf.float32, [n_steps, batch_size, seq_width], name=\"placeholder/seqs\")\n",
    "    # what timesteps we want to stop at, notice it's different for each batch hence dimension of [batch]\n",
    "    early_stop = tf.placeholder(tf.int32, [batch_size], name=\"placeholder/stops\" )\n",
    "\n",
    "    # inputs for rnn needs to be a list, each item being a timestep.\n",
    "    # we need to split our input into each timestep, and reshape it because split keeps dims by default\n",
    "    # input = [n_steps, batch_size, seq_width]\n",
    "    inputs = [tf.reshape(i, (batch_size, seq_width)) for i in tf.split(0, n_steps, seq_input)]\n",
    "    \n",
    "    \"\"\"Shape checker\"\"\"\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     sess.run(init)\n",
    "#     print(sess.run(tf.shape(inputs), feed_dict = {seq_input: np.ones((n_steps, batch_size, seq_width)) }))\n",
    "#     sess.close()\n",
    "    \n",
    "    cell = TensorBasicLSTMCell(size, inp_modes=inp_modes, out_modes=out_modes, mat_ranks=mat_ranks, \n",
    "                          input_size = seq_width , initializer=initializer)        \n",
    "        \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # ========= This is the most important part ==========\n",
    "    # output will be of length 4 and 6\n",
    "    # the state is the final state at termination (stopped at step 4 and 6)\n",
    "    outputs, state = tf.nn.rnn(cell, inputs, initial_state=initial_state, sequence_length=early_stop)\n",
    "\n",
    "    # usual crap\n",
    "    iop = tf.initialize_all_variables()\n",
    "    session = tf.Session()\n",
    "    session.run(iop)\n",
    "    feed = {early_stop:e_stop, seq_input:np.random.rand(n_steps, batch_size, seq_width).astype('float32')}\n",
    "\n",
    "    print(\"outputs, should be 2 things one of length 4 and other of 6\")\n",
    "    outs = session.run(outputs, feed_dict=feed)\n",
    "    for xx in outs:\n",
    "        print(xx)\n",
    "\n",
    "    print(\"states, 2 things total both of size 2, which is the size of the hidden state\")\n",
    "    st = session.run(state, feed_dict=feed)\n",
    "    print(st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
