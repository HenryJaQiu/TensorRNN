{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load '/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py'\n",
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"RNN helpers for TensorFlow models.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import logging_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "\n",
    "def rnn(cell, inputs, initial_state=None, dtype=None,\n",
    "        sequence_length=None, scope=None):\n",
    "\"\"\"Creates a recurrent neural network specified by RNNCell \"cell\".\n",
    "\n",
    "  The simplest form of RNN network generated is:\n",
    "    state = cell.zero_state(...)\n",
    "    outputs = []\n",
    "    for input_ in inputs:\n",
    "      output, state = cell(input_, state)\n",
    "      outputs.append(output)\n",
    "    return (outputs, state)\n",
    "\n",
    "  However, a few other options are available:\n",
    "\n",
    "  An initial state can be provided.\n",
    "  If the sequence_length vector is provided, dynamic calculation is performed.\n",
    "  This method of calculation does not compute the RNN steps past the maximum\n",
    "  sequence length of the minibatch (thus saving computational time),\n",
    "  and properly propagates the state at an example's sequence length\n",
    "  to the final state output.\n",
    "\n",
    "  The dynamic calculation performed is, at time t for batch row b,\n",
    "    (output, state)(b, t) =\n",
    "      (t >= sequence_length(b))\n",
    "        ? (zeros(cell.output_size), states(b, sequence_length(b) - 1))\n",
    "        : cell(input(b, t), state(b, t - 1))\n",
    "\n",
    "  Args:\n",
    "    cell: An instance of RNNCell.\n",
    "    inputs: A length T list of inputs, each a tensor of shape\n",
    "      [batch_size, cell.input_size].\n",
    "    initial_state: (optional) An initial state for the RNN.  This must be\n",
    "      a tensor of appropriate type and shape [batch_size x cell.state_size].\n",
    "    dtype: (optional) The data type for the initial state.  Required if\n",
    "      initial_state is not provided.\n",
    "    sequence_length: Specifies the length of each sequence in inputs.\n",
    "      An int32 or int64 vector (tensor) size [batch_size].  Values in [0, T).\n",
    "    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n",
    "\n",
    "  Returns:\n",
    "    A pair (outputs, state) where:\n",
    "      outputs is a length T list of outputs (one for each input)\n",
    "      state is the final state\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If \"cell\" is not an instance of RNNCell.\n",
    "    ValueError: If inputs is None or an empty list, or if the input depth\n",
    "      cannot be inferred from inputs via shape inference.\n",
    "  \"\"\"\n",
    "    if not isinstance(cell, rnn_cell.RNNCell):\n",
    "        raise TypeError(\"cell must be an instance of RNNCell\")\n",
    "    if not isinstance(inputs, list):\n",
    "        raise TypeError(\"inputs must be a list\")\n",
    "    if not inputs:\n",
    "        raise ValueError(\"inputs must not be empty\")\n",
    "    outputs = []\n",
    "    # Create a new scope in which the caching device is either\n",
    "    # determined by the parent scope, or is set to place the cached\n",
    "    # Variable using the same placement as for the rest of the RNN.\n",
    "    with vs.variable_scope(scope or \"RNN\") as varscope:\n",
    "        if varscope.caching_device is None:\n",
    "            varscope.set_caching_device(lambda op: op.device)\n",
    "\n",
    "    # Temporarily avoid EmbeddingWrapper and seq2seq badness\n",
    "    # TODO(lukaszkaiser): remove EmbeddingWrapper\n",
    "    if inputs[0].get_shape().ndims != 1:\n",
    "        (fixed_batch_size, input_size) = inputs[0].get_shape().with_rank(2)\n",
    "        if input_size.value is None:\n",
    "            raise ValueError(\n",
    "                \"Input size (second dimension of inputs[0]) must be accessible via \"\n",
    "                \"shape inference, but saw value None.\")\n",
    "    else:\n",
    "        fixed_batch_size = inputs[0].get_shape().with_rank_at_least(1)[0]\n",
    "\n",
    "    if fixed_batch_size.value:\n",
    "        batch_size = fixed_batch_size.value\n",
    "    else:\n",
    "        batch_size = array_ops.shape(inputs[0])[0]\n",
    "    if initial_state is not None:\n",
    "        state = initial_state\n",
    "    else:\n",
    "        if not dtype:\n",
    "            raise ValueError(\"If no initial_state is provided, dtype must be.\")\n",
    "        state = cell.zero_state(batch_size, dtype)\n",
    "\n",
    "    if sequence_length is not None:\n",
    "        sequence_length = math_ops.to_int32(sequence_length)\n",
    "\n",
    "    if sequence_length is not None:  # Prepare variables\n",
    "        zero_output = array_ops.zeros(\n",
    "          array_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)\n",
    "        zero_output.set_shape(\n",
    "          tensor_shape.TensorShape([fixed_batch_size.value, cell.output_size]))\n",
    "        min_sequence_length = math_ops.reduce_min(sequence_length)\n",
    "        max_sequence_length = math_ops.reduce_max(sequence_length)\n",
    "\n",
    "    for time, input_ in enumerate(inputs):\n",
    "        if time > 0: vs.get_variable_scope().reuse_variables()\n",
    "        # pylint: disable=cell-var-from-loop\n",
    "        call_cell = lambda: cell(input_, state)\n",
    "        # pylint: enable=cell-var-from-loop\n",
    "        if sequence_length is not None:\n",
    "            (output, state) = _rnn_step(\n",
    "            time, sequence_length, min_sequence_length, max_sequence_length,\n",
    "            zero_output, state, call_cell)\n",
    "        else:\n",
    "            (output, state) = call_cell()\n",
    "\n",
    "        outputs.append(output)\n",
    "\n",
    "    return (outputs, state)\n",
    "\n",
    "\n",
    "def state_saving_rnn(cell, inputs, state_saver, state_name,\n",
    "                     sequence_length=None, scope=None):\n",
    "  \"\"\"RNN that accepts a state saver for time-truncated RNN calculation.\n",
    "\n",
    "  Args:\n",
    "    cell: An instance of RNNCell.\n",
    "    inputs: A length T list of inputs, each a tensor of shape\n",
    "      [batch_size, cell.input_size].\n",
    "    state_saver: A state saver object with methods `state` and `save_state`.\n",
    "    state_name: The name to use with the state_saver.\n",
    "    sequence_length: (optional) An int32/int64 vector size [batch_size].\n",
    "      See the documentation for rnn() for more details about sequence_length.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n",
    "\n",
    "  Returns:\n",
    "    A pair (outputs, state) where:\n",
    "      outputs is a length T list of outputs (one for each input)\n",
    "      states is the final state\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If \"cell\" is not an instance of RNNCell.\n",
    "    ValueError: If inputs is None or an empty list.\n",
    "  \"\"\"\n",
    "  initial_state = state_saver.state(state_name)\n",
    "  (outputs, state) = rnn(cell, inputs, initial_state=initial_state,\n",
    "                         sequence_length=sequence_length, scope=scope)\n",
    "  save_state = state_saver.save_state(state_name, state)\n",
    "  with ops.control_dependencies([save_state]):\n",
    "    outputs[-1] = array_ops.identity(outputs[-1])\n",
    "\n",
    "  return (outputs, state)\n",
    "\n",
    "\n",
    "def _rnn_step(\n",
    "    time, sequence_length, min_sequence_length, max_sequence_length,\n",
    "    zero_output, state, call_cell, skip_conditionals=False):\n",
    "    \"\"\"Calculate one step of a dynamic RNN minibatch.\n",
    "\n",
    "  Returns an (output, state) pair conditioned on the sequence_lengths.\n",
    "  When skip_conditionals=False, the pseudocode is something like:\n",
    "\n",
    "  if t >= max_sequence_length:\n",
    "    return (zero_output, state)\n",
    "  if t < min_sequence_length:\n",
    "    return call_cell()\n",
    "\n",
    "  # Selectively output zeros or output, old state or new state depending\n",
    "  # on if we've finished calculating each row.\n",
    "  new_output, new_state = call_cell()\n",
    "  final_output = np.vstack([\n",
    "    zero_output if time >= sequence_lengths[r] else new_output_r\n",
    "    for r, new_output_r in enumerate(new_output)\n",
    "  ])\n",
    "  final_state = np.vstack([\n",
    "    state[r] if time >= sequence_lengths[r] else new_state_r\n",
    "    for r, new_state_r in enumerate(new_state)\n",
    "  ])\n",
    "  return (final_output, final_state)\n",
    "\n",
    "  Args:\n",
    "    time: Python int, the current time step\n",
    "    sequence_length: int32 `Tensor` vector of size [batch_size]\n",
    "    min_sequence_length: int32 `Tensor` scalar, min of sequence_length\n",
    "    max_sequence_length: int32 `Tensor` scalar, max of sequence_length\n",
    "    zero_output: `Tensor` vector of shape [output_size]\n",
    "    state: `Tensor` matrix of shape [batch_size, state_size]\n",
    "    call_cell: lambda returning tuple of (new_output, new_state) where\n",
    "      new_output is a `Tensor` matrix of shape [batch_size, output_size]\n",
    "      new_state is a `Tensor` matrix of shape [batch_size, state_size]\n",
    "    skip_conditionals: Python bool, whether to skip using the conditional\n",
    "      calculations.  This is useful for dynamic_rnn, where the input tensor\n",
    "      matches max_sequence_length, and using conditionals just slows\n",
    "      everything down.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (final_output, final_state) as given by the pseudocode above:\n",
    "      final_output is a `Tensor` matrix of shape [batch_size, output_size]\n",
    "      final_state is a `Tensor` matrix of shape [batch_size, state_size]\n",
    "  \"\"\"\n",
    "    state_shape = state.get_shape()\n",
    "\n",
    "    def _copy_some_through(new_output, new_state):\n",
    "    # Use broadcasting select to determine which values should get\n",
    "    # the previous state & zero output, and which values should get\n",
    "    # a calculated state & output.\n",
    "        copy_cond = (time >= sequence_length)\n",
    "        return (math_ops.select(copy_cond, zero_output, new_output),\n",
    "            math_ops.select(copy_cond, state, new_state))\n",
    "\n",
    "    def _maybe_copy_some_through():\n",
    "    \"\"\"Run RNN step.  Pass through either no or some past state.\"\"\"\n",
    "        new_output, new_state = call_cell()\n",
    "\n",
    "        return control_flow_ops.cond(\n",
    "         # if t < min_seq_len: calculate and return everything\n",
    "            time < min_sequence_length, lambda: (new_output, new_state),\n",
    "        # else copy some of it through\n",
    "            lambda: _copy_some_through(new_output, new_state))\n",
    "\n",
    "    # TODO(ebrevdo): skipping these conditionals may cause a slowdown,\n",
    "    # but benefits from removing cond() and its gradient.  We should\n",
    "    # profile with and without this switch here.\n",
    "    if skip_conditionals:\n",
    "        # Instead of using conditionals, perform the selective copy at all time\n",
    "        # steps.  This is faster when max_seq_len is equal to the number of unrolls\n",
    "        # (which is typical for dynamic_rnn).\n",
    "        new_output, new_state = call_cell()\n",
    "        (final_output, final_state) = _copy_some_through(new_output, new_state)\n",
    "    else:\n",
    "        empty_update = lambda: (zero_output, state)\n",
    "\n",
    "        (final_output, final_state) = control_flow_ops.cond(\n",
    "            # if t >= max_seq_len: copy all state through, output zeros\n",
    "            time >= max_sequence_length, empty_update,\n",
    "            # otherwise calculation is required: copy some or all of it through\n",
    "            _maybe_copy_some_through)\n",
    "\n",
    "    final_output.set_shape(zero_output.get_shape())\n",
    "    final_state.set_shape(state_shape)\n",
    "    return (final_output, final_state)\n",
    "\n",
    "\n",
    "def _reverse_seq(input_seq, lengths):\n",
    "    \"\"\"Reverse a list of Tensors up to specified lengths.\n",
    "\n",
    "  Args:\n",
    "    input_seq: Sequence of seq_len tensors of dimension (batch_size, depth)\n",
    "    lengths:   A tensor of dimension batch_size, containing lengths for each\n",
    "               sequence in the batch. If \"None\" is specified, simply reverses\n",
    "               the list.\n",
    "\n",
    "  Returns:\n",
    "    time-reversed sequence\n",
    "  \"\"\"\n",
    "  if lengths is None:\n",
    "    return list(reversed(input_seq))\n",
    "\n",
    "  input_shape = tensor_shape.matrix(None, None)\n",
    "  for input_ in input_seq:\n",
    "    input_shape.merge_with(input_.get_shape())\n",
    "    input_.set_shape(input_shape)\n",
    "\n",
    "  # Join into (time, batch_size, depth)\n",
    "  s_joined = array_ops.pack(input_seq)\n",
    "\n",
    "  # TODO(schuster, ebrevdo): Remove cast when reverse_sequence takes int32\n",
    "  if lengths is not None:\n",
    "    lengths = math_ops.to_int64(lengths)\n",
    "\n",
    "  # Reverse along dimension 0\n",
    "  s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)\n",
    "  # Split again into list\n",
    "  result = array_ops.unpack(s_reversed)\n",
    "  for r in result:\n",
    "    r.set_shape(input_shape)\n",
    "  return result\n",
    "\n",
    "\n",
    "def bidirectional_rnn(cell_fw, cell_bw, inputs,\n",
    "                      initial_state_fw=None, initial_state_bw=None,\n",
    "                      dtype=None, sequence_length=None, scope=None):\n",
    "  \"\"\"Creates a bidirectional recurrent neural network.\n",
    "\n",
    "  Similar to the unidirectional case above (rnn) but takes input and builds\n",
    "  independent forward and backward RNNs with the final forward and backward\n",
    "  outputs depth-concatenated, such that the output will have the format\n",
    "  [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of\n",
    "  forward and backward cell must match. The initial state for both directions\n",
    "  is zero by default (but can be set optionally) and no intermediate states are\n",
    "  ever returned -- the network is fully unrolled for the given (passed in)\n",
    "  length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n",
    "\n",
    "  Args:\n",
    "    cell_fw: An instance of RNNCell, to be used for forward direction.\n",
    "    cell_bw: An instance of RNNCell, to be used for backward direction.\n",
    "    inputs: A length T list of inputs, each a tensor of shape\n",
    "      [batch_size, cell.input_size].\n",
    "    initial_state_fw: (optional) An initial state for the forward RNN.\n",
    "      This must be a tensor of appropriate type and shape\n",
    "      [batch_size x cell.state_size].\n",
    "    initial_state_bw: (optional) Same as for initial_state_fw.\n",
    "    dtype: (optional) The data type for the initial state.  Required if either\n",
    "      of the initial states are not provided.\n",
    "    sequence_length: (optional) An int32/int64 vector, size [batch_size],\n",
    "      containing the actual lengths for each of the sequences.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"BiRNN\"\n",
    "\n",
    "  Returns:\n",
    "    A tuple (outputs, output_state_fw, output_state_bw) where:\n",
    "      outputs is a length T list of outputs (one for each input), which\n",
    "      are depth-concatenated forward and backward outputs\n",
    "      output_state_fw is the final state of the forward rnn\n",
    "      output_state_bw is the final state of the backward rnn\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If \"cell_fw\" or \"cell_bw\" is not an instance of RNNCell.\n",
    "    ValueError: If inputs is None or an empty list.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(cell_fw, rnn_cell.RNNCell):\n",
    "    raise TypeError(\"cell_fw must be an instance of RNNCell\")\n",
    "  if not isinstance(cell_bw, rnn_cell.RNNCell):\n",
    "    raise TypeError(\"cell_bw must be an instance of RNNCell\")\n",
    "  if not isinstance(inputs, list):\n",
    "    raise TypeError(\"inputs must be a list\")\n",
    "  if not inputs:\n",
    "    raise ValueError(\"inputs must not be empty\")\n",
    "\n",
    "  name = scope or \"BiRNN\"\n",
    "  # Forward direction\n",
    "  with vs.variable_scope(name + \"_FW\") as fw_scope:\n",
    "    output_fw, output_state_fw = rnn(cell_fw, inputs, initial_state_fw, dtype,\n",
    "                       sequence_length, scope=fw_scope)\n",
    "\n",
    "  # Backward direction\n",
    "  with vs.variable_scope(name + \"_BW\") as bw_scope:\n",
    "    tmp, output_state_bw = rnn(cell_bw, _reverse_seq(inputs, sequence_length),\n",
    "                 initial_state_bw, dtype, sequence_length, scope=bw_scope)\n",
    "  output_bw = _reverse_seq(tmp, sequence_length)\n",
    "  # Concat each of the forward/backward outputs\n",
    "  outputs = [array_ops.concat(1, [fw, bw])\n",
    "             for fw, bw in zip(output_fw, output_bw)]\n",
    "\n",
    "  return (outputs, output_state_fw, output_state_bw)\n",
    "\n",
    "\n",
    "def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None,\n",
    "                dtype=None, parallel_iterations=None, swap_memory=False,\n",
    "                time_major=False, scope=None):\n",
    "  \"\"\"Creates a recurrent neural network specified by RNNCell \"cell\".\n",
    "\n",
    "  This function is functionally identical to the function `rnn` above, but\n",
    "  performs fully dynamic unrolling of `inputs`.\n",
    "\n",
    "  Unlike `rnn`, the input `inputs` is not a Python list of `Tensors`.  Instead,\n",
    "  it is a single `Tensor` where the maximum time is either the first or second\n",
    "  dimension (see the parameter `time_major`).  The corresponding output is\n",
    "  a single `Tensor` having the same number of time steps and batch size.\n",
    "\n",
    "  The parameter `sequence_length` is required and dynamic calculation is\n",
    "  automatically performed.\n",
    "\n",
    "  Args:\n",
    "    cell: An instance of RNNCell.\n",
    "    inputs: The RNN inputs.\n",
    "      If time_major == False (default), this must be a tensor of shape:\n",
    "        `[batch_size, max_time, cell.input_size]`.\n",
    "      If time_major == True, this must be a tensor of shape:\n",
    "        `[max_time, batch_size, cell.input_size]`.\n",
    "    sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.\n",
    "    initial_state: (optional) An initial state for the RNN.  This must be\n",
    "      a tensor of appropriate type and shape `[batch_size x cell.state_size]`.\n",
    "    dtype: (optional) The data type for the initial state.  Required if\n",
    "      initial_state is not provided.\n",
    "    parallel_iterations: (Default: 32).  The number of iterations to run in\n",
    "      parallel.  Those operations which do not have any temporal dependency\n",
    "      and can be run in parallel, will be.  This parameter trades off\n",
    "      time for space.  Values >> 1 use more memory but take less time,\n",
    "      while smaller values use less memory but computations take longer.\n",
    "    swap_memory: Swap the tensors produced in forward inference but needed\n",
    "      for back prop from GPU to CPU.\n",
    "    time_major: The shape format of the `inputs` and `outputs` Tensors.\n",
    "      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "      Using time_major = False is a bit more efficient because it avoids\n",
    "      transposes at the beginning and end of the RNN calculation.  However,\n",
    "      most TensorFlow data is batch-major, so by default this function\n",
    "      accepts input and emits output in batch-major form.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n",
    "\n",
    "  Returns:\n",
    "    A pair (outputs, state) where:\n",
    "      outputs: The RNN output `Tensor`.\n",
    "        If time_major == False (default), this will be a `Tensor` shaped:\n",
    "          `[batch_size, max_time, cell.output_size]`.\n",
    "        If time_major == True, this will be a `Tensor` shaped:\n",
    "          `[max_time, batch_size, cell.output_size]`.\n",
    "      state: The final state, shaped:\n",
    "        `[batch_size, cell.state_size]`.\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If \"cell\" is not an instance of RNNCell.\n",
    "    ValueError: If inputs is None or an empty list.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(cell, rnn_cell.RNNCell):\n",
    "    raise TypeError(\"cell must be an instance of RNNCell\")\n",
    "\n",
    "  # By default, time_major==False and inputs are batch-major: shaped\n",
    "  #   [batch, time, depth]\n",
    "  # For internal calculations, we transpose to [time, batch, depth]\n",
    "  if not time_major:\n",
    "    inputs = array_ops.transpose(inputs, [1, 0, 2])  # (B,T,D) => (T,B,D)\n",
    "\n",
    "  parallel_iterations = parallel_iterations or 32\n",
    "  if sequence_length is not None:\n",
    "    sequence_length = math_ops.to_int32(sequence_length)\n",
    "    sequence_length = array_ops.identity(  # Just to find it in the graph.\n",
    "        sequence_length, name=\"sequence_length\")\n",
    "\n",
    "  # Create a new scope in which the caching device is either\n",
    "  # determined by the parent scope, or is set to place the cached\n",
    "  # Variable using the same placement as for the rest of the RNN.\n",
    "  with vs.variable_scope(scope or \"RNN\") as varscope:\n",
    "    if varscope.caching_device is None:\n",
    "      varscope.set_caching_device(lambda op: op.device)\n",
    "    input_shape = array_ops.shape(inputs)\n",
    "    batch_size = input_shape[1]\n",
    "\n",
    "    if initial_state is not None:\n",
    "      state = initial_state\n",
    "    else:\n",
    "      if not dtype:\n",
    "        raise ValueError(\"If no initial_state is provided, dtype must be.\")\n",
    "      state = cell.zero_state(batch_size, dtype)\n",
    "\n",
    "    def _assert_has_shape(x, shape):\n",
    "      x_shape = array_ops.shape(x)\n",
    "      packed_shape = array_ops.pack(shape)\n",
    "      return logging_ops.Assert(\n",
    "          math_ops.reduce_all(math_ops.equal(x_shape, packed_shape)),\n",
    "          [\"Expected shape for Tensor %s is \" % x.name,\n",
    "           packed_shape, \" but saw shape: \", x_shape])\n",
    "\n",
    "    if sequence_length is not None:\n",
    "      # Perform some shape validation\n",
    "      with ops.control_dependencies(\n",
    "          [_assert_has_shape(sequence_length, [batch_size])]):\n",
    "        sequence_length = array_ops.identity(\n",
    "            sequence_length, name=\"CheckSeqLen\")\n",
    "\n",
    "    (outputs, final_state) = _dynamic_rnn_loop(\n",
    "        cell, inputs, state, parallel_iterations=parallel_iterations,\n",
    "        swap_memory=swap_memory, sequence_length=sequence_length)\n",
    "\n",
    "    # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n",
    "    # If we are performing batch-major calculations, transpose output back\n",
    "    # to shape [batch, time, depth]\n",
    "    if not time_major:\n",
    "      outputs = array_ops.transpose(outputs, [1, 0, 2])  # (T,B,D) => (B,T,D)\n",
    "\n",
    "    return (outputs, final_state)\n",
    "\n",
    "\n",
    "def _dynamic_rnn_loop(\n",
    "    cell, inputs, initial_state, parallel_iterations, swap_memory,\n",
    "    sequence_length=None):\n",
    "  \"\"\"Internal implementation of Dynamic RNN.\n",
    "\n",
    "  Args:\n",
    "    cell: An instance of RNNCell.\n",
    "    inputs: A `Tensor` of shape [time, batch_size, depth].\n",
    "    initial_state: A `Tensor` of shape [batch_size, depth].\n",
    "    parallel_iterations: Positive Python int.\n",
    "    swap_memory: A Python boolean\n",
    "    sequence_length: (optional) An `int32` `Tensor` of shape [batch_size].\n",
    "\n",
    "  Returns:\n",
    "    Tuple (final_outputs, final_state).\n",
    "    final_outputs:\n",
    "      A `Tensor` of shape [time, batch_size, depth]`.\n",
    "    final_state:\n",
    "      A `Tensor` of shape [batch_size, depth].\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the input depth cannot be inferred via shape inference\n",
    "      from the inputs.\n",
    "  \"\"\"\n",
    "  state = initial_state\n",
    "  assert isinstance(parallel_iterations, int), \"parallel_iterations must be int\"\n",
    "\n",
    "  # Construct an initial output\n",
    "  input_shape = array_ops.shape(inputs)\n",
    "  (time_steps, batch_size, _) = array_ops.unpack(input_shape, 3)\n",
    "\n",
    "  inputs_got_shape = inputs.get_shape().with_rank(3)\n",
    "  (const_time_steps, const_batch_size, const_depth) = inputs_got_shape.as_list()\n",
    "\n",
    "  if const_depth is None:\n",
    "    raise ValueError(\n",
    "        \"Input size (depth of inputs) must be accessible via shape inference, \"\n",
    "        \"but saw value None.\")\n",
    "\n",
    "  # Prepare dynamic conditional copying of state & output\n",
    "  zero_output = array_ops.zeros(\n",
    "      array_ops.pack([batch_size, cell.output_size]), inputs.dtype)\n",
    "  if sequence_length is not None:\n",
    "    min_sequence_length = math_ops.reduce_min(sequence_length)\n",
    "    max_sequence_length = math_ops.reduce_max(sequence_length)\n",
    "\n",
    "  time = array_ops.constant(0, dtype=dtypes.int32, name=\"time\")\n",
    "\n",
    "  with ops.op_scope([], \"dynamic_rnn\") as scope:\n",
    "    base_name = scope\n",
    "\n",
    "  output_ta = tensor_array_ops.TensorArray(\n",
    "      dtype=inputs.dtype, size=time_steps,\n",
    "      tensor_array_name=base_name + \"output\")\n",
    "\n",
    "  input_ta = tensor_array_ops.TensorArray(\n",
    "      dtype=inputs.dtype, size=time_steps,\n",
    "      tensor_array_name=base_name + \"input\")\n",
    "\n",
    "  input_ta = input_ta.unpack(inputs)\n",
    "\n",
    "  def _time_step(time, state, output_ta_t):\n",
    "    \"\"\"Take a time step of the dynamic RNN.\n",
    "\n",
    "    Args:\n",
    "      time: int32 scalar Tensor.\n",
    "      state: Vector.\n",
    "      output_ta_t: `TensorArray`, the output with existing flow.\n",
    "\n",
    "    Returns:\n",
    "      The tuple (time + 1, new_state, output_ta_t with updated flow).\n",
    "    \"\"\"\n",
    "\n",
    "    input_t = input_ta.read(time)\n",
    "    # Restore some shape information\n",
    "    input_t.set_shape([const_batch_size, const_depth])\n",
    "\n",
    "    call_cell = lambda: cell(input_t, state)\n",
    "\n",
    "    if sequence_length is not None:\n",
    "      (output, new_state) = _rnn_step(\n",
    "          time=time,\n",
    "          sequence_length=sequence_length,\n",
    "          min_sequence_length=min_sequence_length,\n",
    "          max_sequence_length=max_sequence_length,\n",
    "          zero_output=zero_output,\n",
    "          state=state,\n",
    "          call_cell=call_cell,\n",
    "          skip_conditionals=True)\n",
    "    else:\n",
    "      (output, new_state) = call_cell()\n",
    "\n",
    "    output_ta_t = output_ta_t.write(time, output)\n",
    "\n",
    "    return (time + 1, new_state, output_ta_t)\n",
    "\n",
    "  (_, final_state, output_final_ta) = control_flow_ops.while_loop(\n",
    "      cond=lambda time, _1, _2: time < time_steps,\n",
    "      body=_time_step,\n",
    "      loop_vars=(time, state, output_ta),\n",
    "      parallel_iterations=parallel_iterations,\n",
    "      swap_memory=swap_memory)\n",
    "\n",
    "  final_outputs = output_final_ta.pack()\n",
    "  # Restore some shape information\n",
    "  final_outputs.set_shape([\n",
    "      const_time_steps, const_batch_size, cell.output_size])\n",
    "\n",
    "  return (final_outputs, final_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
