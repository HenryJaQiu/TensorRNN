{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load '/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py'\n",
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Module for constructing RNN Cells.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "\n",
    "class RNNCell(object):\n",
    "  \"\"\"Abstract object representing an RNN cell.\n",
    "\n",
    "  An RNN cell, in the most abstract setting, is anything that has\n",
    "  a state -- a vector of floats of size self.state_size -- and performs some\n",
    "  operation that takes inputs of size self.input_size. This operation\n",
    "  results in an output of size self.output_size and a new state.\n",
    "\n",
    "  This module provides a number of basic commonly used RNN cells, such as\n",
    "  LSTM (Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number\n",
    "  of operators that allow add dropouts, projections, or embeddings for inputs.\n",
    "  Constructing multi-layer cells is supported by a super-class, MultiRNNCell,\n",
    "  defined later. Every RNNCell must have the properties below and and\n",
    "  implement __call__ with the following signature.\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
    "\n",
    "    Args:\n",
    "      inputs: 2D Tensor with shape [batch_size x self.input_size].\n",
    "      state: 2D Tensor with shape [batch_size x self.state_size].\n",
    "      scope: VariableScope for the created subgraph; defaults to class name.\n",
    "\n",
    "    Returns:\n",
    "      A pair containing:\n",
    "      - Output: A 2D Tensor with shape [batch_size x self.output_size]\n",
    "      - New state: A 2D Tensor with shape [batch_size x self.state_size].\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    \"\"\"Integer: size of inputs accepted by this cell.\"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    \"\"\"Integer: size of outputs produced by this cell.\"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"Integer: size of state used by this cell.\"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return state tensor (shape [batch_size x state_size]) filled with 0.\n",
    "\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "\n",
    "    Returns:\n",
    "      A 2D Tensor of shape [batch_size x state_size] filled with zeros.\n",
    "    \"\"\"\n",
    "    zeros = array_ops.zeros(\n",
    "        array_ops.pack([batch_size, self.state_size]), dtype=dtype)\n",
    "    zeros.set_shape([None, self.state_size])\n",
    "    return zeros\n",
    "\n",
    "\n",
    "class BasicRNNCell(RNNCell):\n",
    "  \"\"\"The most basic RNN cell.\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None):\n",
    "    self._num_units = num_units\n",
    "    self._input_size = num_units if input_size is None else input_size\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Most basic RNN: output = new_state = tanh(W * input + U * state + B).\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n",
    "      output = tanh(linear([inputs, state], self._num_units, True))\n",
    "    return output, output\n",
    "\n",
    "\n",
    "class GRUCell(RNNCell):\n",
    "  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None):\n",
    "    self._num_units = num_units\n",
    "    self._input_size = num_units if input_size is None else input_size\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n",
    "      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
    "        # We start with bias of 1.0 to not reset and not update.\n",
    "        r, u = array_ops.split(1, 2, linear([inputs, state],\n",
    "                                            2 * self._num_units, True, 1.0))\n",
    "        r, u = sigmoid(r), sigmoid(u)\n",
    "      with vs.variable_scope(\"Candidate\"):\n",
    "        c = tanh(linear([inputs, r * state], self._num_units, True))\n",
    "      new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h\n",
    "\n",
    "\n",
    "class BasicLSTMCell(RNNCell):\n",
    "  \"\"\"Basic LSTM recurrent network cell.\n",
    "\n",
    "  The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
    "\n",
    "  We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
    "  reduce the scale of forgetting in the beginning of the training.\n",
    "\n",
    "  It does not allow cell clipping, a projection layer, and does not\n",
    "  use peep-hole connections: it is the basic baseline.\n",
    "\n",
    "  For advanced models, please use the full LSTMCell that follows.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, forget_bias=1.0, input_size=None):\n",
    "    \"\"\"Initialize the basic LSTM cell.\n",
    "\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell.\n",
    "      forget_bias: float, The bias added to forget gates (see above).\n",
    "      input_size: int, The dimensionality of the inputs into the LSTM cell,\n",
    "        by default equal to num_units.\n",
    "    \"\"\"\n",
    "    self._num_units = num_units\n",
    "    self._input_size = num_units if input_size is None else input_size\n",
    "    self._forget_bias = forget_bias\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return 2 * self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "      # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "      c, h = array_ops.split(1, 2, state)\n",
    "      concat = linear([inputs, h], 4 * self._num_units, True)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      i, j, f, o = array_ops.split(1, 4, concat)\n",
    "\n",
    "      new_c = c * sigmoid(f + self._forget_bias) + sigmoid(i) * tanh(j)\n",
    "      new_h = tanh(new_c) * sigmoid(o)\n",
    "\n",
    "      return new_h, array_ops.concat(1, [new_c, new_h])\n",
    "\n",
    "\n",
    "def _get_concat_variable(name, shape, dtype, num_shards):\n",
    "  \"\"\"Get a sharded variable concatenated into one tensor.\"\"\"\n",
    "  sharded_variable = _get_sharded_variable(name, shape, dtype, num_shards)\n",
    "  if len(sharded_variable) == 1:\n",
    "    return sharded_variable[0]\n",
    "\n",
    "  concat_name = name + \"/concat\"\n",
    "  concat_full_name = vs.get_variable_scope().name + \"/\" + concat_name + \":0\"\n",
    "  for value in ops.get_collection(ops.GraphKeys.CONCATENATED_VARIABLES):\n",
    "    if value.name == concat_full_name:\n",
    "      return value\n",
    "\n",
    "  concat_variable = array_ops.concat(0, sharded_variable, name=concat_name)\n",
    "  ops.add_to_collection(ops.GraphKeys.CONCATENATED_VARIABLES,\n",
    "                        concat_variable)\n",
    "  return concat_variable\n",
    "\n",
    "\n",
    "def _get_sharded_variable(name, shape, dtype, num_shards):\n",
    "  \"\"\"Get a list of sharded variables with the given dtype.\"\"\"\n",
    "  if num_shards > shape[0]:\n",
    "    raise ValueError(\"Too many shards: shape=%s, num_shards=%d\" %\n",
    "                     (shape, num_shards))\n",
    "  unit_shard_size = int(math.floor(shape[0] / num_shards))\n",
    "  remaining_rows = shape[0] - unit_shard_size * num_shards\n",
    "\n",
    "  shards = []\n",
    "  for i in range(num_shards):\n",
    "    current_size = unit_shard_size\n",
    "    if i < remaining_rows:\n",
    "      current_size += 1\n",
    "    shards.append(vs.get_variable(name + \"_%d\" % i, [current_size] + shape[1:],\n",
    "                                  dtype=dtype))\n",
    "  return shards\n",
    "\n",
    "\n",
    "class LSTMCell(RNNCell):\n",
    "  \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n",
    "\n",
    "  This implementation is based on:\n",
    "\n",
    "    https://research.google.com/pubs/archive/43905.pdf\n",
    "\n",
    "  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n",
    "  \"Long short-term memory recurrent neural network architectures for\n",
    "   large scale acoustic modeling.\" INTERSPEECH, 2014.\n",
    "\n",
    "  It uses peep-hole connections, optional cell clipping, and an optional\n",
    "  projection layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None,\n",
    "               use_peepholes=False, cell_clip=None,\n",
    "               initializer=None, num_proj=None,\n",
    "               num_unit_shards=1, num_proj_shards=1, forget_bias=1.0):\n",
    "    \"\"\"Initialize the parameters for an LSTM cell.\n",
    "\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell\n",
    "      input_size: int, The dimensionality of the inputs into the LSTM cell\n",
    "      use_peepholes: bool, set True to enable diagonal/peephole connections.\n",
    "      cell_clip: (optional) A float value, if provided the cell state is clipped\n",
    "        by this value prior to the cell output activation.\n",
    "      initializer: (optional) The initializer to use for the weight and\n",
    "        projection matrices.\n",
    "      num_proj: (optional) int, The output dimensionality for the projection\n",
    "        matrices.  If None, no projection is performed.\n",
    "      num_unit_shards: How to split the weight matrix.  If >1, the weight\n",
    "        matrix is stored across num_unit_shards.\n",
    "      num_proj_shards: How to split the projection matrix.  If >1, the\n",
    "        projection matrix is stored across num_proj_shards.\n",
    "      forget_bias: Biases of the forget gate are initialized by default to 1\n",
    "        in order to reduce the scale of forgetting at the beginning of the training.\n",
    "    \"\"\"\n",
    "    self._num_units = num_units\n",
    "    self._input_size = input_size\n",
    "    self._use_peepholes = use_peepholes\n",
    "    self._cell_clip = cell_clip\n",
    "    self._initializer = initializer\n",
    "    self._num_proj = num_proj\n",
    "    self._num_unit_shards = num_unit_shards\n",
    "    self._num_proj_shards = num_proj_shards\n",
    "    self._forget_bias = forget_bias\n",
    "\n",
    "    if num_proj:\n",
    "      self._state_size = num_units + num_proj\n",
    "      self._output_size = num_proj\n",
    "    else:\n",
    "      self._state_size = 2 * num_units\n",
    "      self._output_size = num_units\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._num_units if self._input_size is None else self._input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run one step of LSTM.\n",
    "\n",
    "    Args:\n",
    "      inputs: input Tensor, 2D, batch x num_units.\n",
    "      state: state Tensor, 2D, batch x state_size.\n",
    "      scope: VariableScope for the created subgraph; defaults to \"LSTMCell\".\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - A 2D, batch x output_dim, Tensor representing the output of the LSTM\n",
    "        after reading \"inputs\" when previous state was \"state\".\n",
    "        Here output_dim is:\n",
    "           num_proj if num_proj was set,\n",
    "           num_units otherwise.\n",
    "      - A 2D, batch x state_size, Tensor representing the new state of LSTM\n",
    "        after reading \"inputs\" when previous state was \"state\".\n",
    "    Raises:\n",
    "      ValueError: if an input_size was specified and the provided inputs have\n",
    "        a different dimension.\n",
    "    \"\"\"\n",
    "    num_proj = self._num_units if self._num_proj is None else self._num_proj\n",
    "\n",
    "    c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])\n",
    "    m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])\n",
    "\n",
    "    dtype = inputs.dtype\n",
    "    actual_input_size = inputs.get_shape().as_list()[1]\n",
    "    if self._input_size and self._input_size != actual_input_size:\n",
    "      raise ValueError(\"Actual input size not same as specified: %d vs %d.\" %\n",
    "                       (actual_input_size, self._input_size))\n",
    "    with vs.variable_scope(scope or type(self).__name__,\n",
    "                           initializer=self._initializer):  # \"LSTMCell\"\n",
    "      concat_w = _get_concat_variable(\n",
    "          \"W\", [actual_input_size + num_proj, 4 * self._num_units],\n",
    "          dtype, self._num_unit_shards)\n",
    "\n",
    "      b = vs.get_variable(\n",
    "          \"B\", shape=[4 * self._num_units],\n",
    "          initializer=array_ops.zeros_initializer, dtype=dtype)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      cell_inputs = array_ops.concat(1, [inputs, m_prev])\n",
    "      lstm_matrix = nn_ops.bias_add(math_ops.matmul(cell_inputs, concat_w), b)\n",
    "      i, j, f, o = array_ops.split(1, 4, lstm_matrix)\n",
    "\n",
    "      # Diagonal connections\n",
    "      if self._use_peepholes:\n",
    "        w_f_diag = vs.get_variable(\n",
    "            \"W_F_diag\", shape=[self._num_units], dtype=dtype)\n",
    "        w_i_diag = vs.get_variable(\n",
    "            \"W_I_diag\", shape=[self._num_units], dtype=dtype)\n",
    "        w_o_diag = vs.get_variable(\n",
    "            \"W_O_diag\", shape=[self._num_units], dtype=dtype)\n",
    "\n",
    "      if self._use_peepholes:\n",
    "        c = (sigmoid(f + self._forget_bias + w_f_diag * c_prev) * c_prev +\n",
    "             sigmoid(i + w_i_diag * c_prev) * tanh(j))\n",
    "      else:\n",
    "        c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * tanh(j))\n",
    "\n",
    "      if self._cell_clip is not None:\n",
    "        c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)\n",
    "\n",
    "      if self._use_peepholes:\n",
    "        m = sigmoid(o + w_o_diag * c) * tanh(c)\n",
    "      else:\n",
    "        m = sigmoid(o) * tanh(c)\n",
    "\n",
    "      if self._num_proj is not None:\n",
    "        concat_w_proj = _get_concat_variable(\n",
    "            \"W_P\", [self._num_units, self._num_proj],\n",
    "            dtype, self._num_proj_shards)\n",
    "\n",
    "        m = math_ops.matmul(m, concat_w_proj)\n",
    "\n",
    "    return m, array_ops.concat(1, [c, m])\n",
    "\n",
    "\n",
    "class OutputProjectionWrapper(RNNCell):\n",
    "  \"\"\"Operator adding an output projection to the given cell.\n",
    "\n",
    "  Note: in many cases it may be more efficient to not use this wrapper,\n",
    "  but instead concatenate the whole sequence of your outputs in time,\n",
    "  do the projection on this batch-concatenated sequence, then split it\n",
    "  if needed or directly feed into a softmax.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, cell, output_size):\n",
    "    \"\"\"Create a cell with output projection.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell, a projection to output_size is added to it.\n",
    "      output_size: integer, the size of the output after projection.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "      ValueError: if output_size is not positive.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not RNNCell.\")\n",
    "    if output_size < 1:\n",
    "      raise ValueError(\"Parameter output_size must be > 0: %d.\" % output_size)\n",
    "    self._cell = cell\n",
    "    self._output_size = output_size\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._cell.input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the cell and output projection on inputs, starting from state.\"\"\"\n",
    "    output, res_state = self._cell(inputs, state)\n",
    "    # Default scope: \"OutputProjectionWrapper\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):\n",
    "      projected = linear(output, self._output_size, True)\n",
    "    return projected, res_state\n",
    "\n",
    "\n",
    "class InputProjectionWrapper(RNNCell):\n",
    "  \"\"\"Operator adding an input projection to the given cell.\n",
    "\n",
    "  Note: in many cases it may be more efficient to not use this wrapper,\n",
    "  but instead concatenate the whole sequence of your inputs in time,\n",
    "  do the projection on this batch-concatenated sequence, then split it.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, cell, input_size):\n",
    "    \"\"\"Create a cell with input projection.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell, a projection of inputs is added before it.\n",
    "      input_size: integer, the size of the inputs before projection.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "      ValueError: if input_size is not positive.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not RNNCell.\")\n",
    "    if input_size < 1:\n",
    "      raise ValueError(\"Parameter input_size must be > 0: %d.\" % input_size)\n",
    "    self._cell = cell\n",
    "    self._input_size = input_size\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the input projection and then the cell.\"\"\"\n",
    "    # Default scope: \"InputProjectionWrapper\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):\n",
    "      projected = linear(inputs, self._cell.input_size, True)\n",
    "    return self._cell(projected, state)\n",
    "\n",
    "\n",
    "class DropoutWrapper(RNNCell):\n",
    "  \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n",
    "\n",
    "  def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0,\n",
    "               seed=None):\n",
    "    \"\"\"Create a cell with added input and/or output dropout.\n",
    "\n",
    "    Dropout is never used on the state.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell, a projection to output_size is added to it.\n",
    "      input_keep_prob: unit Tensor or float between 0 and 1, input keep\n",
    "        probability; if it is float and 1, no input dropout will be added.\n",
    "      output_keep_prob: unit Tensor or float between 0 and 1, output keep\n",
    "        probability; if it is float and 1, no output dropout will be added.\n",
    "      seed: (optional) integer, the randomness seed.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "      ValueError: if keep_prob is not between 0 and 1.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not a RNNCell.\")\n",
    "    if (isinstance(input_keep_prob, float) and\n",
    "        not (input_keep_prob >= 0.0 and input_keep_prob <= 1.0)):\n",
    "      raise ValueError(\"Parameter input_keep_prob must be between 0 and 1: %d\"\n",
    "                       % input_keep_prob)\n",
    "    if (isinstance(output_keep_prob, float) and\n",
    "        not (output_keep_prob >= 0.0 and output_keep_prob <= 1.0)):\n",
    "      raise ValueError(\"Parameter input_keep_prob must be between 0 and 1: %d\"\n",
    "                       % output_keep_prob)\n",
    "    self._cell = cell\n",
    "    self._input_keep_prob = input_keep_prob\n",
    "    self._output_keep_prob = output_keep_prob\n",
    "    self._seed = seed\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._cell.input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the cell with the declared dropouts.\"\"\"\n",
    "    if (not isinstance(self._input_keep_prob, float) or\n",
    "        self._input_keep_prob < 1):\n",
    "      inputs = nn_ops.dropout(inputs, self._input_keep_prob, seed=self._seed)\n",
    "    output, new_state = self._cell(inputs, state)\n",
    "    if (not isinstance(self._output_keep_prob, float) or\n",
    "        self._output_keep_prob < 1):\n",
    "      output = nn_ops.dropout(output, self._output_keep_prob, seed=self._seed)\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "class EmbeddingWrapper(RNNCell):\n",
    "  \"\"\"Operator adding input embedding to the given cell.\n",
    "\n",
    "  Note: in many cases it may be more efficient to not use this wrapper,\n",
    "  but instead concatenate the whole sequence of your inputs in time,\n",
    "  do the embedding on this batch-concatenated sequence, then split it and\n",
    "  feed into your RNN.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, cell, embedding_classes, embedding_size, initializer=None):\n",
    "    \"\"\"Create a cell with an added input embedding.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell, an embedding will be put before its inputs.\n",
    "      embedding_classes: integer, how many symbols will be embedded.\n",
    "      embedding_size: integer, the size of the vectors we embed into.\n",
    "      initializer: an initializer to use when creating the embedding;\n",
    "        if None, the initializer from variable scope or a default one is used.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "      ValueError: if embedding_classes is not positive.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not RNNCell.\")\n",
    "    if embedding_classes <= 0 or embedding_size <= 0:\n",
    "      raise ValueError(\"Both embedding_classes and embedding_size must be > 0: \"\n",
    "                       \"%d, %d.\" % (embedding_classes, embedding_size))\n",
    "    self._cell = cell\n",
    "    self._embedding_classes = embedding_classes\n",
    "    self._embedding_size = embedding_size\n",
    "    self._initializer = initializer\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return 1\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the cell on embedded inputs.\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"EmbeddingWrapper\"\n",
    "      with ops.device(\"/cpu:0\"):\n",
    "        if self._initializer:\n",
    "          initializer = self._initializer\n",
    "        elif vs.get_variable_scope().initializer:\n",
    "          initializer = vs.get_variable_scope().initializer\n",
    "        else:\n",
    "          # Default initializer for embeddings should have variance=1.\n",
    "          sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "          initializer = init_ops.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "        embedding = vs.get_variable(\"embedding\", [self._embedding_classes,\n",
    "                                                  self._embedding_size],\n",
    "                                    initializer=initializer)\n",
    "        embedded = embedding_ops.embedding_lookup(\n",
    "            embedding, array_ops.reshape(inputs, [-1]))\n",
    "    return self._cell(embedded, state)\n",
    "\n",
    "\n",
    "class MultiRNNCell(RNNCell):\n",
    "  \"\"\"RNN cell composed sequentially of multiple simple cells.\"\"\"\n",
    "\n",
    "  def __init__(self, cells):\n",
    "    \"\"\"Create a RNN cell composed sequentially of a number of RNNCells.\n",
    "\n",
    "    Args:\n",
    "      cells: list of RNNCells that will be composed in this order.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if cells is empty (not allowed) or if their sizes don't match.\n",
    "    \"\"\"\n",
    "    if not cells:\n",
    "      raise ValueError(\"Must specify at least one cell for MultiRNNCell.\")\n",
    "    for i in xrange(len(cells) - 1):\n",
    "      if cells[i + 1].input_size != cells[i].output_size:\n",
    "        raise ValueError(\"In MultiRNNCell, the input size of each next\"\n",
    "                         \" cell must match the output size of the previous one.\"\n",
    "                         \" Mismatched output size in cell %d.\" % i)\n",
    "    self._cells = cells\n",
    "\n",
    "  @property\n",
    "  def input_size(self):\n",
    "    return self._cells[0].input_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cells[-1].output_size\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return sum([cell.state_size for cell in self._cells])\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"MultiRNNCell\"\n",
    "      cur_state_pos = 0\n",
    "      cur_inp = inputs\n",
    "      new_states = []\n",
    "      for i, cell in enumerate(self._cells):\n",
    "        with vs.variable_scope(\"Cell%d\" % i):\n",
    "          cur_state = array_ops.slice(\n",
    "              state, [0, cur_state_pos], [-1, cell.state_size])\n",
    "          cur_state_pos += cell.state_size\n",
    "          cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "          new_states.append(new_state)\n",
    "    return cur_inp, array_ops.concat(1, new_states)\n",
    "\n",
    "\n",
    "class SlimRNNCell(RNNCell):\n",
    "  \"\"\"A simple wrapper for slim.rnn_cells.\"\"\"\n",
    "\n",
    "  def __init__(self, cell_fn):\n",
    "    \"\"\"Create a SlimRNNCell from a cell_fn.\n",
    "\n",
    "    Args:\n",
    "      cell_fn: a function which takes (inputs, state, scope) and produces the\n",
    "        outputs and the new_state. Additionally when called with inputs=None and\n",
    "        state=None it should return (initial_outputs, initial_state).\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell_fn is not callable\n",
    "      ValueError: if cell_fn cannot produce a valid initial state.\n",
    "    \"\"\"\n",
    "    if not callable(cell_fn):\n",
    "      raise TypeError(\"cell_fn %s needs to be callable\", cell_fn)\n",
    "    self._cell_fn = cell_fn\n",
    "    self._cell_name = cell_fn.func.__name__\n",
    "    _, init_state = self._cell_fn(None, None)\n",
    "    state_shape = init_state.get_shape()\n",
    "    self._state_size = state_shape.with_rank(2)[1].value\n",
    "    if self._state_size is None:\n",
    "      raise ValueError(\"Initial state created by %s has invalid shape %s\",\n",
    "                       self._cell_name, state_shape)\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._state_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    scope = scope or self._cell_name\n",
    "    output, state = self._cell_fn(inputs, state, scope=scope)\n",
    "    return output, state\n",
    "\n",
    "\n",
    "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "  Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "  if args is None or (isinstance(args, (list, tuple)) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not isinstance(args, (list, tuple)):\n",
    "    args = [args]\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape().as_list() for a in args]\n",
    "  for shape in shapes:\n",
    "    if len(shape) != 2:\n",
    "      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "    if not shape[1]:\n",
    "      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "    else:\n",
    "      total_arg_size += shape[1]\n",
    "\n",
    "  # Now the computation.\n",
    "  with vs.variable_scope(scope or \"Linear\"):\n",
    "    matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size])\n",
    "    if len(args) == 1:\n",
    "      res = math_ops.matmul(args[0], matrix)\n",
    "    else:\n",
    "      res = math_ops.matmul(array_ops.concat(1, args), matrix)\n",
    "    if not bias:\n",
    "      return res\n",
    "    bias_term = vs.get_variable(\n",
    "        \"Bias\", [output_size],\n",
    "        initializer=init_ops.constant_initializer(bias_start))\n",
    "  return res + bias_term\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
