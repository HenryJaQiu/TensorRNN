{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq 2 seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp steps 12 out steps None\n",
      "loading time series ...\n",
      "input type  <class 'numpy.ndarray'> (1000, 100, 1)\n",
      "normalize to (0-1)\n"
     ]
    }
   ],
   "source": [
    "# %load train_seq2seq.py\n",
    "\"\"\"Functions for downloading and reading time series data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import random_seed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from reader import read_data_sets\n",
    "from model_seq2seq import *\n",
    "from trnn import *\n",
    "import numpy \n",
    "from train_config import *\n",
    "\n",
    "\n",
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"model\", \"HOALSTM\",\n",
    "          \"Model used for learning.\")\n",
    "flags.DEFINE_string(\"data_path\", \"./data.npy\",\n",
    "          \"Data input directory.\")\n",
    "flags.DEFINE_string(\"save_path\", \"./log/lstm/\",\n",
    "          \"Model output directory.\")\n",
    "flags.DEFINE_bool(\"use_error_prop\", True,\n",
    "                  \"Feed previous output as input in RNN\")\n",
    "flags.DEFINE_bool(\"use_sched_samp\", False,\n",
    "                  \"Use scheduled sampling in training\")\n",
    "flags.DEFINE_integer(\"burn_in_steps\", 12, \"burn in steps\")\n",
    "flags.DEFINE_integer(\"test_steps\", None, \"test steps size\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 64, \"hidden layer size\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-2, \"learning rate\")\n",
    "flags.DEFINE_float(\"decay_rate\", 0.8, \"learning rate\")\n",
    "flags.DEFINE_integer(\"rank\", 2, \"rank for tt decomposition\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "'''\n",
    "To forecast time series using a recurrent neural network, we consider every \n",
    "row as a sequence of short time series. Because dataset times series has 9 dim, we will then\n",
    "handle 9 sequences for every sample.\n",
    "'''\n",
    "\n",
    "# Training Parameters\n",
    "config = TrainConfig()\n",
    "config.use_error_prop = FLAGS.use_error_prop\n",
    "config.burn_in_steps = FLAGS.burn_in_steps\n",
    "config.test_steps = FLAGS.test_steps\n",
    "config.hidden_size = FLAGS.hidden_size\n",
    "config.learning_rate = FLAGS.learning_rate\n",
    "config.decay_rate = FLAGS.decay_rate\n",
    "config.rank_vals = [FLAGS.rank]\n",
    "\n",
    "# Scheduled sampling\n",
    "# = tf.Variable(0.0, trainable=False)\n",
    "if FLAGS.use_sched_samp:\n",
    "    config.sample_prob = tf.get_variable(\"sample_prob\", shape=(), initializer=tf.zeros_initializer())\n",
    "sampling_burn_in = 400\n",
    "\n",
    "# Training Parameters\n",
    "training_steps = config.training_steps\n",
    "batch_size = config.batch_size\n",
    "display_step = 200\n",
    "inp_steps = config.burn_in_steps\n",
    "test_steps = config.test_steps\n",
    "\n",
    "# Read Dataset\n",
    "print('inp steps', inp_steps, 'out steps', test_steps)\n",
    "dataset, stats = read_data_sets(FLAGS.data_path, True, inp_steps, test_steps)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = stats['num_input']  # dataset data input (time series dimension: 3)\n",
    "out_steps = test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HighOrderLSTMCell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9bff452b4991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roseyu/Documents/Python/TensorRNN/experiments/model_seq2seq.py\u001b[0m in \u001b[0;36mHOALSTM\u001b[0;34m(enc_inps, dec_inps, is_training, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhoalstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mHighOrderAugLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_orders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhoalstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         cell = tf.contrib.rnn.DropoutWrapper(\n",
      "\u001b[0;32m/Users/roseyu/Documents/Python/TensorRNN/experiments/model_seq2seq.py\u001b[0m in \u001b[0;36mhoalstm_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mHOALSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhoalstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mHighOrderAugLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_orders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhoalstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roseyu/Documents/Python/TensorRNN/experiments/trnn_aug.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_units, num_lags, num_orders, forget_bias, state_is_tuple, activation, reuse)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"\"\"LSTM cell with high-order interactions of hidden states\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHighOrderLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_lags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_lags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HighOrderLSTMCell' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, inp_steps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "\n",
    "# Decoder output\n",
    "Z = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "\n",
    "Model = globals()[FLAGS.model]\n",
    "with tf.name_scope(\"Train\"):\n",
    "    with tf.variable_scope(\"Model\", reuse=None):\n",
    "        train_pred = Model(X, Y, True,  config)\n",
    "with tf.name_scope(\"Test\"):\n",
    "    with tf.variable_scope(\"Model\", reuse=True):\n",
    "        test_pred = Model(X, Y, False,  config)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "train_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(train_pred, Z)))\n",
    "test_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(test_pred, Z)))\n",
    "# Exponential learning rate decay \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = config.learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           2000, config.decay_rate, staircase=True)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(train_loss,global_step=global_step)\n",
    "\n",
    "# Scheduled sampling params\n",
    "eps_min = 0.1 # minimal prob\n",
    "\n",
    "# Write summary\n",
    "train_summary = tf.summary.scalar('train_loss', train_loss)\n",
    "valid_summary = tf.summary.scalar('valid_loss', test_loss)\n",
    "lr_summary = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "hist_loss =[]\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.save_path,sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_z = dataset.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, Z:batch_z})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss \n",
    "            summary, loss = sess.run([merged,train_loss], feed_dict={X: batch_x,Y: batch_y, Z:batch_z})\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary_writer.add_run_metadata(run_metadata, 'step%03d' % step)\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) )\n",
    "            \n",
    "            # Calculate validation\n",
    "            valid_enc_inps = dataset.validation.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "            valid_dec_inps = dataset.validation.dec_inps.reshape((-1, out_steps, num_input))\n",
    "            valid_dec_outs = dataset.validation.dec_outs.reshape((-1, out_steps, num_input))\n",
    "            va_sum, va_loss = sess.run([valid_summary,test_loss], \\\n",
    "                                       feed_dict={X: valid_enc_inps, Y: valid_dec_inps, Z: valid_dec_outs})\n",
    "            summary_writer.add_summary(va_sum, step) \n",
    "            print(\"Validation Loss:\", va_loss)\n",
    "            \n",
    "            # Overfitting\n",
    "            hist_loss.append(va_loss)\n",
    "            if len(hist_loss)>20 and va_loss > np.mean(hist_loss):\n",
    "                print(\"Early stopping: step \", step)\n",
    "                break\n",
    "          \n",
    "            #Update sampling prob\n",
    "            if FLAGS.use_sched_samp and step > sampling_burn_in:\n",
    "                sample_prob = max(eps_min, 1.0-step/(2*training_steps))\n",
    "                sess.run(tf.assign(config.sample_prob, sample_prob))\n",
    "                print('Sampling prob:', sample_prob)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for test datasets\n",
    "    test_enc_inps = dataset.test.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "    test_dec_inps = dataset.test.dec_inps.reshape((-1, out_steps, num_input))\n",
    "    test_dec_outs = dataset.test.dec_outs.reshape((-1, out_steps, num_input))\n",
    "\n",
    "    \n",
    "    # Fetch the predictions \n",
    "    fetches = {\n",
    "        \"true\":Z,\n",
    "        \"pred\":test_pred,\n",
    "        \"loss\":test_loss\n",
    "    }\n",
    "    test_vals = sess.run(fetches, feed_dict={X: test_enc_inps, Y: test_dec_inps, Z: test_dec_outs})\n",
    "    print(\"Testing Loss:\", test_vals[\"loss\"])\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, FLAGS.save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    # Save predictions \n",
    "    numpy.save(save_path+\"predict.npy\", (test_vals[\"true\"], test_vals[\"pred\"]))\n",
    "    # Save config file\n",
    "    with open(save_path+\"config.out\", 'w') as f:\n",
    "        f.write('hidden_size:'+ str(config.hidden_size)+'\\t'+ 'learning_rate:'+ str(config.learning_rate)+ '\\n')\n",
    "        f.write('train_error:'+ str(loss) +'\\t'+ 'test_error:'+ str(test_vals[\"loss\"]) +'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
