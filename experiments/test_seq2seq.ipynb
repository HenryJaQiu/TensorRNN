{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq 2 seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading time series ...\n",
      "input type  <class 'numpy.ndarray'> (8784, 288, 15)\n",
      "normalize to (0-1)\n",
      "Creating model @ training  --> Not using scheduled sampling.\n",
      "                               --> Feeding ground truth into input.\n",
      "Creating model @ training  --> Using scheduled sampling.\n",
      "                               --> Feeding ground truth into input.\n",
      "Creating model @ training  --> Not using scheduled sampling.\n",
      "                               --> Feeding ground truth into input.\n",
      "Creating model @ training  --> Not using scheduled sampling.\n",
      "                               --> Feeding output back into input.\n",
      "Step 1, Minibatch Loss= 0.3967\n",
      "Validation Loss: 0.389865\n",
      "Step 200, Minibatch Loss= 0.2915\n",
      "Validation Loss: 0.294491\n",
      "Step 400, Minibatch Loss= 0.1591\n",
      "Validation Loss: 0.165937\n",
      "Step 600, Minibatch Loss= 0.3101\n",
      "Validation Loss: 0.308042\n",
      "Step 800, Minibatch Loss= 0.1537\n",
      "Validation Loss: 0.180432\n",
      "Step 1000, Minibatch Loss= 0.0881\n",
      "Validation Loss: 0.0974866\n",
      "Step 1200, Minibatch Loss= 0.0883\n",
      "Validation Loss: 0.091376\n",
      "Step 1400, Minibatch Loss= 0.0772\n",
      "Validation Loss: 0.083843\n",
      "Step 1600, Minibatch Loss= 0.0767\n",
      "Validation Loss: 0.0805468\n",
      "Step 1800, Minibatch Loss= 0.0798\n",
      "Validation Loss: 0.0783121\n",
      "Step 2000, Minibatch Loss= 0.0830\n",
      "Validation Loss: 0.0870348\n",
      "Step 2200, Minibatch Loss= 0.0786\n",
      "Validation Loss: 0.0785918\n",
      "Step 2400, Minibatch Loss= 0.0732\n",
      "Validation Loss: 0.0766194\n",
      "Step 2600, Minibatch Loss= 0.0593\n",
      "Validation Loss: 0.0748245\n",
      "Step 2800, Minibatch Loss= 0.0694\n",
      "Validation Loss: 0.0787025\n",
      "Step 3000, Minibatch Loss= 0.0737\n",
      "Validation Loss: 0.0742209\n",
      "Step 3200, Minibatch Loss= 0.0698\n",
      "Validation Loss: 0.0750732\n",
      "Step 3400, Minibatch Loss= 0.0646\n",
      "Validation Loss: 0.0741029\n",
      "Step 3600, Minibatch Loss= 0.0807\n",
      "Validation Loss: 0.0759707\n",
      "Step 3800, Minibatch Loss= 0.0689\n",
      "Validation Loss: 0.0735869\n",
      "Step 4000, Minibatch Loss= 0.0697\n",
      "Validation Loss: 0.073489\n",
      "Step 4200, Minibatch Loss= 0.0748\n",
      "Validation Loss: 0.0731755\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16258c5873f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_z\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Calculate batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load train_seq2seq.py\n",
    "\"\"\"Functions for downloading and reading time series data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import random_seed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from reader import read_data_sets\n",
    "from model_seq2seq import *\n",
    "from trnn import *\n",
    "import numpy \n",
    "from train_config import *\n",
    "\n",
    "\n",
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"model\", \"LSTM\",\n",
    "          \"Model used for learning.\")\n",
    "flags.DEFINE_string(\"data_path\", \"/home/qiyu/data/traffic_s2s.npy\",\n",
    "          \"Data input directory.\")\n",
    "flags.DEFINE_string(\"save_path\", \"./log/lstm/\",\n",
    "          \"Model output directory.\")\n",
    "flags.DEFINE_bool(\"use_error_prop\", True,\n",
    "                  \"Feed previous output as input in RNN\")\n",
    "flags.DEFINE_bool(\"use_sched_samp\", False,\n",
    "                  \"Use scheduled sampling in training\")\n",
    "flags.DEFINE_integer(\"burn_in_steps\", 12, \"burn in steps\")\n",
    "flags.DEFINE_integer(\"test_steps\", 8, \"hidden layer size\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 64, \"hidden layer size\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-2, \"learning rate\")\n",
    "flags.DEFINE_float(\"decay_rate\", 0.8, \"learning rate\")\n",
    "flags.DEFINE_integer(\"rank\", 2, \"rank for tt decomposition\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "'''\n",
    "To forecast time series using a recurrent neural network, we consider every \n",
    "row as a sequence of short time series. Because dataset times series has 9 dim, we will then\n",
    "handle 9 sequences for every sample.\n",
    "'''\n",
    "\n",
    "# Training Parameters\n",
    "config = TrainConfig()\n",
    "config.use_error_prop = FLAGS.use_error_prop\n",
    "config.burn_in_steps = FLAGS.burn_in_steps\n",
    "config.test_steps = FLAGS.test_steps\n",
    "config.hidden_size = FLAGS.hidden_size\n",
    "config.learning_rate = FLAGS.learning_rate\n",
    "config.decay_rate = FLAGS.decay_rate\n",
    "config.rank_vals = [FLAGS.rank]\n",
    "\n",
    "# Scheduled sampling\n",
    "# = tf.Variable(0.0, trainable=False)\n",
    "if FLAGS.use_sched_samp:\n",
    "    config.sample_prob = tf.get_variable(\"sample_prob\", shape=(), initializer=tf.zeros_initializer())\n",
    "sampling_burn_in = 400\n",
    "\n",
    "# Training Parameters\n",
    "training_steps = config.training_steps\n",
    "batch_size = config.batch_size\n",
    "display_step = 200\n",
    "inp_steps = config.burn_in_steps\n",
    "test_steps = config.test_steps\n",
    "\n",
    "# Read Dataset\n",
    "dataset, stats = read_data_sets(FLAGS.data_path, True, inp_steps, test_steps)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = stats['num_input']  # dataset data input (time series dimension: 3)\n",
    "out_steps = test_steps+1 # adding EOS\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, inp_steps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "\n",
    "# Decoder output\n",
    "Z = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "\n",
    "Model = globals()[FLAGS.model]\n",
    "with tf.name_scope(\"Train\"):\n",
    "    with tf.variable_scope(\"Model\", reuse=None):\n",
    "        train_pred = Model(X, Y, True,  config)\n",
    "with tf.name_scope(\"Test\"):\n",
    "    with tf.variable_scope(\"Model\", reuse=True):\n",
    "        test_pred = Model(X, Y, False,  config)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "train_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(train_pred, Z)))\n",
    "test_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(test_pred, Z)))\n",
    "# Exponential learning rate decay \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = config.learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           2000, config.decay_rate, staircase=True)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(train_loss,global_step=global_step)\n",
    "\n",
    "# Scheduled sampling params\n",
    "eps_min = 0.1 # minimal prob\n",
    "\n",
    "# Write summary\n",
    "train_summary = tf.summary.scalar('train_loss', train_loss)\n",
    "valid_summary = tf.summary.scalar('valid_loss', test_loss)\n",
    "lr_summary = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "hist_loss =[]\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.save_path,sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_z = dataset.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, Z:batch_z})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss \n",
    "            summary, loss = sess.run([merged,train_loss], feed_dict={X: batch_x,Y: batch_y, Z:batch_z})\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary_writer.add_run_metadata(run_metadata, 'step%03d' % step)\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) )\n",
    "            \n",
    "            # Calculate validation\n",
    "            valid_enc_inps = dataset.validation.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "            valid_dec_inps = dataset.validation.dec_inps.reshape((-1, out_steps, num_input))\n",
    "            valid_dec_outs = dataset.validation.dec_outs.reshape((-1, out_steps, num_input))\n",
    "            va_sum, va_loss = sess.run([valid_summary,test_loss], \\\n",
    "                                       feed_dict={X: valid_enc_inps, Y: valid_dec_inps, Z: valid_dec_outs})\n",
    "            summary_writer.add_summary(va_sum, step) \n",
    "            print(\"Validation Loss:\", va_loss)\n",
    "            \n",
    "            # Overfitting\n",
    "            hist_loss.append(va_loss)\n",
    "            if len(hist_loss)>20 and va_loss > np.mean(hist_loss):\n",
    "                print(\"Early stopping: step \", step)\n",
    "                break\n",
    "          \n",
    "            #Update sampling prob\n",
    "            if FLAGS.use_sched_samp and step > sampling_burn_in:\n",
    "                sample_prob = max(eps_min, 1.0-step/(2*training_steps))\n",
    "                sess.run(tf.assign(config.sample_prob, sample_prob))\n",
    "                print('Sampling prob:', sample_prob)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for test datasets\n",
    "    test_enc_inps = dataset.test.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "    test_dec_inps = dataset.test.dec_inps.reshape((-1, out_steps, num_input))\n",
    "    test_dec_outs = dataset.test.dec_outs.reshape((-1, out_steps, num_input))\n",
    "\n",
    "    \n",
    "    # Fetch the predictions \n",
    "    fetches = {\n",
    "        \"true\":Z,\n",
    "        \"pred\":test_pred,\n",
    "        \"loss\":test_loss\n",
    "    }\n",
    "    test_vals = sess.run(fetches, feed_dict={X: test_enc_inps, Y: test_dec_inps, Z: test_dec_outs})\n",
    "    print(\"Testing Loss:\", test_vals[\"loss\"])\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, FLAGS.save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    # Save predictions \n",
    "    numpy.save(save_path+\"predict.npy\", (test_vals[\"true\"], test_vals[\"pred\"]))\n",
    "    # Save config file\n",
    "    with open(save_path+\"config.out\", 'w') as f:\n",
    "        f.write('hidden_size:'+ str(config.hidden_size)+'\\t'+ 'learning_rate:'+ str(config.learning_rate)+ '\\n')\n",
    "        f.write('train_error:'+ str(loss) +'\\t'+ 'test_error:'+ str(test_vals[\"loss\"]) +'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
